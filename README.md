# TestPilot AI Engine üöó

Motor de inteligencia artificial para generaci√≥n de tests de autoescuela basado en RAG (Retrieval-Augmented Generation).

## √çndice

- [Requisitos Previos](#requisitos-previos)
- [Instalaci√≥n](#instalaci√≥n)
- [Configuraci√≥n](#configuraci√≥n)
- [Uso desde Terminal](#uso-desde-terminal)
- [Documentaci√≥n Interactiva](#documentaci√≥n-interactiva)
- [Endpoints de la API](#endpoints-de-la-api)
- [Ejemplos de Uso](#ejemplos-de-uso)

---

## Requisitos Previos

Antes de comenzar, aseg√∫rate de tener instalado:

- **Python 3.10+**
- **Ollama** con un modelo LLM descargado (por defecto: `gemma2:9b`)

### Instalar Ollama

```bash
# Windows (PowerShell como administrador)
winget install Ollama.Ollama

# Una vez instalado, descarga el modelo
ollama pull gemma2:9b
```

---

## Instalaci√≥n

### 1. Clonar el repositorio

```bash
git clone <url-del-repositorio>
cd test-pilot-ai
```

### 2. Crear entorno virtual

```bash
# Crear entorno virtual
python -m venv venv

# Activar entorno virtual
# Windows (PowerShell)
.\venv\Scripts\Activate.ps1

# Windows (CMD)
venv\Scripts\activate.bat

# Linux/Mac
source venv/bin/activate
```

### 3. Instalar dependencias

```bash
pip install -r requirements.txt
```

> **Nota para actualizaciones:** Si ya ten√≠as una versi√≥n anterior instalada, ejecuta:
> ```bash
> pip install -r requirements.txt --upgrade
> ```

### 4. Configurar variables de entorno

```bash
# Copiar archivo de ejemplo
cp .env.example .env

# Editar seg√∫n necesidades (opcional)
```

---

## Configuraci√≥n

El archivo `.env` permite personalizar el comportamiento del sistema:

| Variable | Descripci√≥n | Valor por defecto |
|----------|-------------|-------------------|
| `LLM_MODEL` | Modelo de Ollama a utilizar | `gemma2:9b` |
| `LLM_TEMPERATURE` | Creatividad del modelo (0.0-1.0) | `0.7` |
| `CHUNK_SIZE` | Tama√±o de chunks al procesar PDFs | `1000` |
| `CHUNK_OVERLAP` | Solapamiento entre chunks | `200` |
| `DEFAULT_SEARCH_K` | Chunks a recuperar por b√∫squeda | `4` |
| `EMBEDDINGS_MODEL` | Modelo de embeddings | `sentence-transformers/all-MiniLM-L6-v2` |
| `CORS_ORIGINS` | Or√≠genes permitidos para CORS | `http://localhost:3000` |
| `CHROMA_PATH` | Ruta de la base de datos vectorial | `vector_db` |

---

## Uso desde Terminal

### Iniciar el servidor

```bash
# Desarrollo (con recarga autom√°tica)
uvicorn app.main:app --reload

# Producci√≥n
uvicorn app.main:app --host 0.0.0.0 --port 8000
```

El servidor estar√° disponible en: `http://localhost:8000`

### Verificar estado del servidor

```bash
# Comprobar que el servidor est√° funcionando
curl http://localhost:8000/

# Health check
curl http://localhost:8000/health-check
```

### Subir un manual PDF

```bash
curl -X POST "http://localhost:8000/admin/ai/upload-manual" \
  -F "file=@/ruta/al/manual.pdf" \
  -F "topic=se√±ales_trafico"
```

### Buscar en la base de datos

```bash
curl "http://localhost:8000/admin/ai/test-search?query=velocidad%20maxima&topic=se√±ales_trafico"
```

### Generar una pregunta

```bash
curl "http://localhost:8000/admin/ai/generate-question?topic=se√±ales_trafico"
```

### Generar un test completo

```bash
# Test de 10 preguntas (por defecto)
curl "http://localhost:8000/admin/ai/generate-full-test?topic=se√±ales_trafico"

# Test personalizado (20 preguntas)
curl "http://localhost:8000/admin/ai/generate-full-test?topic=se√±ales_trafico&num_questions=20"
```

### Resetear la base de datos

```bash
curl -X DELETE "http://localhost:8000/admin/ai/reset-db"
```

---

## Documentaci√≥n Interactiva

FastAPI genera documentaci√≥n autom√°tica e interactiva. Una vez el servidor est√© corriendo, accede a:

### Swagger UI
üìç **http://localhost:8000/docs**

Interfaz interactiva que permite:
- Ver todos los endpoints disponibles
- Probar cada endpoint directamente desde el navegador
- Ver los esquemas de request/response
- Descargar la especificaci√≥n OpenAPI

### ReDoc
üìç **http://localhost:8000/redoc**

Documentaci√≥n alternativa con un dise√±o m√°s limpio, ideal para consulta r√°pida.

### OpenAPI JSON
üìç **http://localhost:8000/openapi.json**

Especificaci√≥n OpenAPI 3.0 en formato JSON, √∫til para:
- Generar clientes autom√°ticamente
- Importar en Postman/Insomnia
- Integraci√≥n con otras herramientas

---

## Endpoints de la API

### `GET /`
Verifica que el servidor est√° funcionando.

**Respuesta:**
```json
{
  "status": "online",
  "message": "TestPilot AI Engine funcionando"
}
```

---

### `GET /health-check`
Comprueba el estado del modelo LLM.

**Respuesta:**
```json
{
  "model": "gemma2",
  "ollama_status": "ready"
}
```

---

### `POST /admin/ai/upload-manual`
Sube y procesa un archivo PDF para alimentar la base de conocimientos.

**Par√°metros (form-data):**
| Campo | Tipo | Descripci√≥n |
|-------|------|-------------|
| `file` | File | Archivo PDF a procesar |
| `topic` | String | Tema/categor√≠a del contenido |

**Respuesta exitosa:**
```json
{
  "message": "Archivo 'manual_dgt.pdf' procesado con √©xito",
  "topic": "se√±ales_trafico",
  "chunks_created": 45
}
```

---

### `GET /admin/ai/test-search`
Busca contenido relevante en la base de datos vectorial.

**Par√°metros (query):**
| Campo | Tipo | Descripci√≥n |
|-------|------|-------------|
| `query` | String | Texto de b√∫squeda |
| `topic` | String | Tema donde buscar |

**Respuesta:**
```json
{
  "query": "l√≠mite de velocidad",
  "topic": "se√±ales_trafico",
  "results_found": 4,
  "data": [
    {
      "content": "El l√≠mite de velocidad en v√≠as urbanas...",
      "metadata": { "topic": "se√±ales_trafico", "source": "manual.pdf" }
    }
  ]
}
```

---

### `GET /admin/ai/generate-question`
Genera una pregunta de test basada en el contenido almacenado.

**Par√°metros (query):**
| Campo | Tipo | Descripci√≥n |
|-------|------|-------------|
| `topic` | String | Tema del que generar la pregunta |

**Respuesta:**
```json
{
  "pregunta": "¬øCu√°l es la velocidad m√°xima en v√≠as urbanas?",
  "opciones": ["30 km/h", "50 km/h", "80 km/h"],
  "respuesta_correcta": "50 km/h",
  "explicacion": "Seg√∫n el Reglamento General de Circulaci√≥n..."
}
```

---

### `GET /admin/ai/generate-full-test`
Genera un test completo con m√∫ltiples preguntas.

**Par√°metros (query):**
| Campo | Tipo | Descripci√≥n |
|-------|------|-------------|
| `topic` | String | Tema del test |
| `num_questions` | Integer | N√∫mero de preguntas (default: 10) |

**Respuesta:**
```json
{
  "topic": "se√±ales_trafico",
  "total_questions": 10,
  "test": [
    {
      "pregunta": "...",
      "opciones": ["...", "...", "..."],
      "respuesta_correcta": "...",
      "explicacion": "..."
    }
  ]
}
```

---

### `DELETE /admin/ai/reset-db`
Elimina todos los documentos de la base de datos vectorial.

**Respuesta:**
```json
{
  "message": "Base de datos vectorial reseteada con √©xito"
}
```

---

## Ejemplos de Uso

### Flujo t√≠pico de trabajo

```bash
# 1. Iniciar Ollama (en otra terminal)
ollama serve

# 2. Iniciar el servidor
uvicorn app.main:app --reload

# 3. Subir un manual PDF
curl -X POST "http://localhost:8000/admin/ai/upload-manual" \
  -F "file=@manual_dgt.pdf" \
  -F "topic=normas_generales"

# 4. Generar un test de 15 preguntas
curl "http://localhost:8000/admin/ai/generate-full-test?topic=normas_generales&num_questions=15"
```

### Usando Python (requests)

```python
import requests

BASE_URL = "http://localhost:8000"

# Subir PDF
with open("manual.pdf", "rb") as f:
    response = requests.post(
        f"{BASE_URL}/admin/ai/upload-manual",
        files={"file": f},
        data={"topic": "se√±ales"}
    )
    print(response.json())

# Generar test
response = requests.get(
    f"{BASE_URL}/admin/ai/generate-full-test",
    params={"topic": "se√±ales", "num_questions": 5}
)
test = response.json()
print(f"Test generado con {test['total_questions']} preguntas")
```

### Usando JavaScript (fetch)

```javascript
// Generar una pregunta
const response = await fetch(
  'http://localhost:8000/admin/ai/generate-question?topic=se√±ales_trafico'
);
const question = await response.json();
console.log(question);
```

---

## Arquitectura

```
test-pilot-ai/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ main.py           # Punto de entrada FastAPI
‚îÇ   ‚îú‚îÄ‚îÄ config.py         # Configuraci√≥n centralizada
‚îÇ   ‚îú‚îÄ‚îÄ routes/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ admin_ai.py   # Endpoints de la API
‚îÇ   ‚îî‚îÄ‚îÄ services/
‚îÇ       ‚îú‚îÄ‚îÄ llm_service.py   # L√≥gica de generaci√≥n con LLM
‚îÇ       ‚îî‚îÄ‚îÄ rag_service.py   # L√≥gica de RAG y vectores
‚îú‚îÄ‚îÄ vector_db/            # Base de datos ChromaDB
‚îú‚îÄ‚îÄ .env                  # Variables de entorno
‚îú‚îÄ‚îÄ .env.example          # Plantilla de configuraci√≥n
‚îî‚îÄ‚îÄ requirements.txt      # Dependencias Python
```

---

## Tecnolog√≠as

- **FastAPI** - Framework web moderno y r√°pido
- **LangChain** - Orquestaci√≥n de LLMs y RAG
- **Ollama** - Ejecuci√≥n local de modelos LLM
- **ChromaDB** - Base de datos vectorial (via `langchain-chroma`)
- **HuggingFace** - Modelos de embeddings

---

## Licencia

MIT License