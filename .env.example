# ===========================================
# TestPilot AI Engine - Configuración
# ===========================================
# Copia este archivo como .env y ajusta los valores según tu entorno

# --- Configuración del LLM ---
# Modelo de Ollama a utilizar (ej: gemma2:9b, llama3:8b, mistral)
LLM_MODEL=gemma2:9b
# Temperatura del modelo (0.0 = determinístico, 1.0 = creativo)
LLM_TEMPERATURE=0.7

# --- Configuración del RAG ---
# Tamaño de cada chunk al procesar PDFs (en caracteres)
CHUNK_SIZE=1000
# Solapamiento entre chunks consecutivos
CHUNK_OVERLAP=200
# Número de chunks similares a recuperar por defecto
DEFAULT_SEARCH_K=4
# Modelo de embeddings de HuggingFace
EMBEDDINGS_MODEL=sentence-transformers/all-MiniLM-L6-v2

# --- Configuración del servidor ---
# Orígenes permitidos para CORS (separar múltiples con coma)
CORS_ORIGINS=http://localhost:3000
# Directorio temporal para uploads
UPLOAD_DIR=temp_uploads

# --- Base de datos vectorial ---
# Ruta al directorio de ChromaDB
CHROMA_PATH=vector_db
